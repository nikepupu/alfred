{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3613jvsc74a57bd031146510740782b1bc15b20863bd6d0a0dc62b272327c4675f88c0b4f1de465b",
   "display_name": "Python 3.6.13 64-bit ('alfred': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "31146510740782b1bc15b20863bd6d0a0dc62b272327c4675f88c0b4f1de465b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb3342bd078>"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import json\n",
    "from gen.utils.py_util import remove_spaces_and_lower\n",
    "from vocab import Vocab\n",
    "import revtok\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "import models.nn.vnn as vnn\n",
    "import torch. nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset.json', 'r') as fp:\n",
    "    data_train = json.load(fp)\n",
    "with open('dataset_valid_seen.json', 'r') as fp:\n",
    "    data_valid_seen = json.load(fp)\n",
    "with open('dataset_valid_unseen.json', 'r') as fp:\n",
    "    data_valid_unseen = json.load(fp)\n",
    "with open('dataset'+'_high_plan_train'+'.json', 'r') as fp:\n",
    "    data_train_high_plan = json.load(fp)\n",
    "with open('dataset'+'_high_plan_valid_seen'+'.json', 'r') as fp:\n",
    "    data_valid_seen_high_plan = json.load(fp)\n",
    "with open('dataset'+'_high_plan_valid_unseen'+'.json', 'r') as fp:\n",
    "    data_valid_unseen_high_plan = json.load(fp)\n",
    "\n",
    "with open('dataset'+'_high_plan_argument_train'+'.json', 'r') as fp:\n",
    "    data_train_high_plan_argument = json.load(fp)\n",
    "with open('dataset'+'_high_plan_argument_valid_seen'+'.json', 'r') as fp:\n",
    "    data_valid_seen_high_plan_argument = json.load(fp)\n",
    "with open('dataset'+'_high_plan_argument_valid_unseen'+'.json', 'r') as fp:\n",
    "    data_valid_unseen_high_plan_argument = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['look_at_obj_in_light']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['CleanObject', 'CoolObject', 'GotoLocation', 'HeatObject', 'PickupObject', 'PutObject', 'SliceObject', 'ToggleObject']\n",
      "['CleanObject', 'CoolObject', 'GotoLocation', 'HeatObject', 'PickupObject', 'PutObject', 'SliceObject', 'ToggleObject']\n",
      "['CleanObject', 'CoolObject', 'GotoLocation', 'HeatObject', 'PickupObject', 'PutObject', 'SliceObject', 'ToggleObject']\n",
      "['alarmclock', 'apple', 'armchair', 'baseballbat', 'basketball', 'bathtubbasin', 'bed', 'book', 'bottle', 'bowl', 'box', 'bread', 'butterknife', 'cabinet', 'candle', 'cart', 'cd', 'cellphone', 'cloth', 'coffeemachine', 'coffeetable', 'countertop', 'creditcard', 'cup', 'desk', 'desklamp', 'diningtable', 'dishsponge', 'drawer', 'dresser', 'egg', 'floorlamp', 'fork', 'fridge', 'garbagecan', 'handtowel', 'handtowelholder', 'kettle', 'keychain', 'knife', 'ladle', 'laptop', 'lettuce', 'microwave', 'mug', 'newspaper', 'ottoman', 'pan', 'pen', 'pencil', 'peppershaker', 'pillow', 'plate', 'plunger', 'pot', 'potato', 'remotecontrol', 'safe', 'saltshaker', 'shelf', 'sidetable', 'sinkbasin', 'soapbar', 'sofa', 'spatula', 'spoon', 'statue', 'stoveburner', 'tennisracket', 'tissuebox', 'toilet', 'toiletpaper', 'toiletpaperhanger', 'tomato', 'vase', 'watch', 'wateringcan']\n",
      "['alarmclock', 'apple', 'armchair', 'baseballbat', 'basketball', 'bathtubbasin', 'bed', 'book', 'bottle', 'bowl', 'box', 'bread', 'butterknife', 'cabinet', 'candle', 'cart', 'cd', 'cellphone', 'cloth', 'coffeemachine', 'coffeetable', 'countertop', 'creditcard', 'cup', 'desk', 'desklamp', 'diningtable', 'dishsponge', 'drawer', 'dresser', 'egg', 'floorlamp', 'fork', 'fridge', 'garbagecan', 'handtowel', 'handtowelholder', 'kettle', 'keychain', 'knife', 'ladle', 'laptop', 'lettuce', 'microwave', 'mug', 'newspaper', 'ottoman', 'pan', 'pen', 'pencil', 'peppershaker', 'pillow', 'plate', 'plunger', 'pot', 'potato', 'remotecontrol', 'safe', 'saltshaker', 'shelf', 'sidetable', 'sinkbasin', 'soapbar', 'sofa', 'spatula', 'spoon', 'statue', 'stoveburner', 'tennisracket', 'tissuebox', 'toilet', 'toiletpaper', 'toiletpaperhanger', 'tomato', 'vase', 'watch', 'wateringcan']\n",
      "['alarmclock', 'apple', 'armchair', 'baseballbat', 'basketball', 'bathtubbasin', 'bed', 'book', 'bottle', 'bowl', 'box', 'bread', 'butterknife', 'cabinet', 'candle', 'cart', 'cd', 'cellphone', 'cloth', 'coffeemachine', 'coffeetable', 'countertop', 'creditcard', 'cup', 'desk', 'desklamp', 'diningtable', 'dishsponge', 'drawer', 'dresser', 'egg', 'floorlamp', 'fork', 'fridge', 'garbagecan', 'handtowel', 'handtowelholder', 'kettle', 'keychain', 'knife', 'ladle', 'laptop', 'lettuce', 'microwave', 'mug', 'newspaper', 'ottoman', 'pan', 'pen', 'pencil', 'peppershaker', 'pillow', 'plate', 'plunger', 'pot', 'potato', 'remotecontrol', 'safe', 'saltshaker', 'shelf', 'sidetable', 'sinkbasin', 'soapbar', 'sofa', 'spatula', 'spoon', 'statue', 'stoveburner', 'tennisracket', 'tissuebox', 'toilet', 'toiletpaper', 'toiletpaperhanger', 'tomato', 'vase', 'watch', 'wateringcan']\n"
     ]
    }
   ],
   "source": [
    "class HighPlanClassificationDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset, labels = None):\n",
    "      \n",
    "        self.dataset = dataset\n",
    "        if labels:\n",
    "            self.label = labels\n",
    "        else:\n",
    "            self.label = list(dataset.keys())\n",
    "        self.label.sort()\n",
    "        print(self.label)\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.vocab = torch.load('pp.vocab')\n",
    "        self.pad = 0\n",
    "        self.seg = 1\n",
    "\n",
    "\n",
    "        for key in dataset.keys():\n",
    "         \n",
    "            for item in dataset[key]:\n",
    "                traj = {}\n",
    "                traj['ann']  = {\n",
    "                    'goal': revtok.tokenize(remove_spaces_and_lower(item))\n",
    "                }\n",
    "                traj['num'] = {}\n",
    "                traj['num']['lang_goal'] = self.numericalize(self.vocab['word'], traj['ann']['goal'], train=True)\n",
    "            \n",
    "                lang_goal = traj['num']['lang_goal']\n",
    "                \n",
    "                # lang_goal_instr = lang_goal + lang_instr\n",
    "                # lang_goal_instr = lang_instr\n",
    "\n",
    "                lang_goal_instr = lang_goal\n",
    "                traj['num']['lang_goal_instr'] = lang_goal_instr\n",
    "\n",
    "                # for k, v in traj['num'].items():\n",
    "                #             if k in {'lang_goal_instr'}:\n",
    "                #                 # language embedding and padding\n",
    "                #                 seqs = [torch.tensor(vv, device=device) for vv in v]\n",
    "                #                 pad_seq = pad_sequence(seqs, batch_first=True, padding_value=self.pad)\n",
    "                #                 seq_lengths = np.array(list(map(len, v)))\n",
    "                #                 embed_seq = self.emb_word(pad_seq)\n",
    "                #                 packed_input = pack_padded_sequence(embed_seq, seq_lengths, batch_first=True, enforce_sorted=False)\n",
    "                #                 traj['num'][k] = packed_input\n",
    "\n",
    "                self.data.append( lang_goal_instr  )\n",
    "                self.labels.append(key)\n",
    "                \n",
    "\n",
    "    def serialize_lang_action(self, feat):\n",
    "        '''\n",
    "        append segmented instr language and low-level actions into single sequences\n",
    "        '''\n",
    "        is_serialized = not isinstance(feat['num']['lang_instr'][0], list)\n",
    "        if not is_serialized:\n",
    "            feat['num']['lang_instr'] = [word for desc in feat['num']['lang_instr'] for word in desc]\n",
    "\n",
    "    @staticmethod\n",
    "    def numericalize(vocab, words, train=True):\n",
    "        '''\n",
    "        converts words to unique integers\n",
    "        '''\n",
    "        return vocab.word2index([w.strip().lower() for w in words], train=train)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "     \n",
    "        \n",
    "        return self.data[idx], self.label.index(self.labels[idx])\n",
    "\n",
    "train_plan = HighPlanClassificationDataset(data_train_high_plan)\n",
    "valid_seen_plan = HighPlanClassificationDataset(data_valid_seen_high_plan)\n",
    "valid_unseen_plan = HighPlanClassificationDataset(data_valid_unseen_high_plan)\n",
    "\n",
    "train_plan_argument = HighPlanClassificationDataset(data_train_high_plan_argument)\n",
    "valid_seen_plan_argument = HighPlanClassificationDataset(data_valid_seen_high_plan_argument, train_plan_argument.label)\n",
    "valid_unseen_plan_argument = HighPlanClassificationDataset(data_valid_unseen_high_plan_argument, train_plan_argument.label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "len(train_plan_argument.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TaskClassificationDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "      \n",
    "        self.dataset = dataset\n",
    "        self.label = list(dataset.keys())\n",
    "        self.label.sort()\n",
    "        print(self.label)\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.vocab = torch.load('pp.vocab')\n",
    "        self.pad = 0\n",
    "        self.seg = 1\n",
    "\n",
    "\n",
    "        for key in dataset.keys():\n",
    "            for item in dataset[key]:\n",
    "                for p in item:\n",
    "                    traj = {}\n",
    "                    traj['ann']  = {\n",
    "                        'goal': revtok.tokenize(remove_spaces_and_lower(p['task_desc'])) + ['<<goal>>'],\n",
    "                        'instr': [revtok.tokenize(remove_spaces_and_lower(x)) for x in p['high_descs']] + [['<<stop>>']],\n",
    "                    }\n",
    "                    traj['num'] = {}\n",
    "                    traj['num']['lang_goal'] = self.numericalize(self.vocab['word'], traj['ann']['goal'], train=True)\n",
    "                    traj['num']['lang_instr'] = [self.numericalize(self.vocab['word'], x, train=True) for x in traj['ann']['instr']]\n",
    "                    self.serialize_lang_action(traj)\n",
    "                    lang_goal, lang_instr = traj['num']['lang_goal'], traj['num']['lang_instr']\n",
    "                    lang_goal_instr = lang_goal + lang_instr\n",
    "                    # lang_goal_instr = lang_instr\n",
    "                    # lang_goal_instr = lang_goal\n",
    "                    traj['num']['lang_goal_instr'] = lang_goal_instr\n",
    "                    # for k, v in traj['num'].items():\n",
    "                    #             if k in {'lang_goal_instr'}:\n",
    "                    #                 # language embedding and padding\n",
    "                    #                 seqs = [torch.tensor(vv, device=device) for vv in v]\n",
    "                    #                 pad_seq = pad_sequence(seqs, batch_first=True, padding_value=self.pad)\n",
    "                    #                 seq_lengths = np.array(list(map(len, v)))\n",
    "                    #                 embed_seq = self.emb_word(pad_seq)\n",
    "                    #                 packed_input = pack_padded_sequence(embed_seq, seq_lengths, batch_first=True, enforce_sorted=False)\n",
    "                    #                 traj['num'][k] = packed_input\n",
    "\n",
    "                    self.data.append( lang_goal_instr  )\n",
    "\n",
    "                    self.labels.append(key)\n",
    "\n",
    "    def serialize_lang_action(self, feat):\n",
    "        '''\n",
    "        append segmented instr language and low-level actions into single sequences\n",
    "        '''\n",
    "        is_serialized = not isinstance(feat['num']['lang_instr'][0], list)\n",
    "        if not is_serialized:\n",
    "            feat['num']['lang_instr'] = [word for desc in feat['num']['lang_instr'] for word in desc]\n",
    "\n",
    "    @staticmethod\n",
    "    def numericalize(vocab, words, train=True):\n",
    "        '''\n",
    "        converts words to unique integers\n",
    "        '''\n",
    "        return vocab.word2index([w.strip().lower() for w in words], train=train)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "     \n",
    "        \n",
    "        return self.data[idx], self.label.index(self.labels[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['look_at_obj_in_light', 'pick_and_place_simple', 'pick_and_place_with_movable_recep', 'pick_clean_then_place_in_recep', 'pick_cool_then_place_in_recep', 'pick_heat_then_place_in_recep', 'pick_two_obj_and_place']\n",
      "['look_at_obj_in_light', 'pick_and_place_simple', 'pick_and_place_with_movable_recep', 'pick_clean_then_place_in_recep', 'pick_cool_then_place_in_recep', 'pick_heat_then_place_in_recep', 'pick_two_obj_and_place']\n",
      "['look_at_obj_in_light', 'pick_and_place_simple', 'pick_and_place_with_movable_recep', 'pick_clean_then_place_in_recep', 'pick_cool_then_place_in_recep', 'pick_heat_then_place_in_recep', 'pick_two_obj_and_place']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_set = TaskClassificationDataset(data_train )\n",
    "val_set1 = TaskClassificationDataset(data_valid_seen )\n",
    "val_set2 = TaskClassificationDataset(data_valid_unseen)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_output=7):\n",
    "        super().__init__()\n",
    "        self.lang_dropout = nn.Dropout(0.3, inplace=True)\n",
    "        self.enc = nn.LSTM(512, 512, bidirectional=True, batch_first=True)\n",
    "        self.enc_att = vnn.SelfAttn(512 * 2)\n",
    "        self.fc1 = nn.Linear(1024,num_output)\n",
    "       \n",
    "        self.emb_word = nn.Embedding(len(train_set.vocab['word']), 512)\n",
    "    def forward(self, x):\n",
    "        emb_lang_goal_instr = x\n",
    "        self.lang_dropout(emb_lang_goal_instr.data)\n",
    "        enc_lang_goal_instr, _ = self.enc(emb_lang_goal_instr)\n",
    "        enc_lang_goal_instr, _ = pad_packed_sequence(enc_lang_goal_instr, batch_first=True)\n",
    "        self.lang_dropout(enc_lang_goal_instr)\n",
    "        cont_lang_goal_instr = self.enc_att(enc_lang_goal_instr)\n",
    "        res = self.fc1(cont_lang_goal_instr)\n",
    "        res = nn.functional.relu(res)\n",
    "\n",
    "        # res = self.fc2(res)\n",
    "        # res = nn.functional.relu(res)\n",
    "\n",
    "        res = F.log_softmax(res, dim=1)\n",
    "        return res\n",
    "model = Net().cuda()\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "        device = 'cuda'\n",
    "        lang_goal_instr, gt = zip(*batch)\n",
    "        seqs = [torch.tensor(vv, device=device) for vv in lang_goal_instr]\n",
    "        pad_seq = pad_sequence(seqs, batch_first=True, padding_value=0)\n",
    "        seq_lengths = np.array(list(map(len, lang_goal_instr)))\n",
    "        embed_seq = model.emb_word(pad_seq)\n",
    "        packed_input = pack_padded_sequence(embed_seq, seq_lengths, batch_first=True, enforce_sorted=False)\n",
    "        lang_goal_instr = packed_input\n",
    "        return lang_goal_instr, gt \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "21025\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "train_loader = DataLoader( train_set, batch_size=100, collate_fn=collate_fn, shuffle=True, num_workers=0)\n",
    "val_loader1 = DataLoader( val_set1, batch_size=100, collate_fn=collate_fn, shuffle=False, num_workers=0)\n",
    "val_loader2 = DataLoader( val_set2, batch_size=100, collate_fn=collate_fn, shuffle=False, num_workers=0)\n",
    "\n",
    "train_loader_plan = DataLoader( train_plan, batch_size=100, collate_fn=collate_fn, shuffle=True, num_workers=0)\n",
    "valid_seen_loader_plan = DataLoader( valid_seen_plan, batch_size=100, collate_fn=collate_fn, shuffle=True, num_workers=0)\n",
    "valid_unseen_loader_plan = DataLoader( valid_unseen_plan, batch_size=100, collate_fn=collate_fn, shuffle=True, num_workers=0)\n",
    "\n",
    "train_loader_plan_argument = DataLoader( train_plan_argument, batch_size=200, collate_fn=collate_fn, shuffle=True, num_workers=0)\n",
    "valid_seen_loader_plan_argument = DataLoader( valid_seen_plan_argument, batch_size=200, collate_fn=collate_fn, shuffle=True, num_workers=0)\n",
    "valid_unseen_loader_plan_argument = DataLoader( valid_unseen_plan_argument, batch_size=200, collate_fn=collate_fn, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(num_output=7).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-67b688e4cc60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_goal_instr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/alfred/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/alfred/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "epochs = 20\n",
    "optimizer = torch.optim.AdamW(model.parameters(), 3e-5)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, 50, len(train_loader)*epochs)\n",
    "for epoch  in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    model.train()\n",
    "    for i, sample_batched in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        lang_goal_instr, gt = sample_batched\n",
    "        target = torch.LongTensor(gt).cuda()\n",
    "        output = model(lang_goal_instr.cuda())\n",
    "        loss =  F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        running_loss += loss.item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    print('[%d, %5d] loss: %.3f correct: %.3f' % (epoch + 1, i + 1, running_loss, correct/len(train_loader.dataset) ))\n",
    "    # print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / len(train_loader.dataset)))\n",
    "            \n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i, sample_batched in enumerate(val_loader1):\n",
    "           \n",
    "\n",
    "            lang_goal_instr, gt = sample_batched\n",
    "            target = torch.LongTensor(gt).cuda()\n",
    "            output = model(lang_goal_instr.cuda())\n",
    "\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        test_loss /= len(val_loader1.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(val_loader1.dataset),\n",
    "    100. * correct / len(val_loader1.dataset)))\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i, sample_batched in enumerate(val_loader2):\n",
    "          \n",
    "\n",
    "            lang_goal_instr, gt = sample_batched\n",
    "            target = torch.LongTensor(gt).cuda()\n",
    "            output = model(lang_goal_instr.cuda())\n",
    "\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        test_loss /= len(val_loader2.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(val_loader2.dataset),\n",
    "    100. * correct / len(val_loader2.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1,  1413] loss: 591.551 correct: 0.881\n",
      "\n",
      "Test set: Average loss: 0.1374, Accuracy: 5368/5570 (96%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1565, Accuracy: 4936/5140 (96%)\n",
      "\n",
      "[2,  1413] loss: 159.562 correct: 0.976\n",
      "\n",
      "Test set: Average loss: 0.1142, Accuracy: 5476/5570 (98%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1238, Accuracy: 5039/5140 (98%)\n",
      "\n",
      "[3,  1413] loss: 107.833 correct: 0.986\n",
      "\n",
      "Test set: Average loss: 0.0706, Accuracy: 5492/5570 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0783, Accuracy: 5063/5140 (99%)\n",
      "\n",
      "[4,  1413] loss: 76.420 correct: 0.990\n",
      "\n",
      "Test set: Average loss: 0.0677, Accuracy: 5498/5570 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0744, Accuracy: 5068/5140 (99%)\n",
      "\n",
      "[5,  1413] loss: 71.349 correct: 0.991\n",
      "\n",
      "Test set: Average loss: 0.0646, Accuracy: 5500/5570 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0709, Accuracy: 5069/5140 (99%)\n",
      "\n",
      "[6,  1413] loss: 67.647 correct: 0.991\n",
      "\n",
      "Test set: Average loss: 0.0623, Accuracy: 5505/5570 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0683, Accuracy: 5069/5140 (99%)\n",
      "\n",
      "[7,  1413] loss: 65.607 correct: 0.991\n",
      "\n",
      "Test set: Average loss: 0.0610, Accuracy: 5505/5570 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0682, Accuracy: 5071/5140 (99%)\n",
      "\n",
      "[8,  1413] loss: 62.922 correct: 0.991\n",
      "\n",
      "Test set: Average loss: 0.0603, Accuracy: 5505/5570 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0658, Accuracy: 5068/5140 (99%)\n",
      "\n",
      "[9,  1413] loss: 60.987 correct: 0.992\n",
      "\n",
      "Test set: Average loss: 0.0598, Accuracy: 5505/5570 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0648, Accuracy: 5070/5140 (99%)\n",
      "\n",
      "[10,  1413] loss: 59.571 correct: 0.992\n",
      "\n",
      "Test set: Average loss: 0.0594, Accuracy: 5507/5570 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0643, Accuracy: 5072/5140 (99%)\n",
      "\n",
      "[11,  1413] loss: 58.370 correct: 0.992\n",
      "\n",
      "Test set: Average loss: 0.0589, Accuracy: 5508/5570 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0631, Accuracy: 5073/5140 (99%)\n",
      "\n",
      "[12,  1413] loss: 56.732 correct: 0.992\n",
      "\n",
      "Test set: Average loss: 0.0581, Accuracy: 5508/5570 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0640, Accuracy: 5073/5140 (99%)\n",
      "\n",
      "[13,  1413] loss: 56.080 correct: 0.992\n",
      "\n",
      "Test set: Average loss: 0.0581, Accuracy: 5508/5570 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0626, Accuracy: 5068/5140 (99%)\n",
      "\n",
      "[14,  1413] loss: 55.055 correct: 0.992\n",
      "\n",
      "Test set: Average loss: 0.0576, Accuracy: 5509/5570 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0623, Accuracy: 5068/5140 (99%)\n",
      "\n",
      "[15,  1413] loss: 54.348 correct: 0.993\n",
      "\n",
      "Test set: Average loss: 0.0575, Accuracy: 5508/5570 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0621, Accuracy: 5070/5140 (99%)\n",
      "\n",
      "[16,  1413] loss: 53.121 correct: 0.993\n",
      "\n",
      "Test set: Average loss: 0.0570, Accuracy: 5510/5570 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0622, Accuracy: 5072/5140 (99%)\n",
      "\n",
      "[17,  1413] loss: 52.895 correct: 0.993\n",
      "\n",
      "Test set: Average loss: 0.0571, Accuracy: 5508/5570 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0613, Accuracy: 5069/5140 (99%)\n",
      "\n",
      "[18,  1413] loss: 52.294 correct: 0.993\n",
      "\n",
      "Test set: Average loss: 0.0570, Accuracy: 5508/5570 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0616, Accuracy: 5068/5140 (99%)\n",
      "\n",
      "[19,  1413] loss: 51.994 correct: 0.993\n",
      "\n",
      "Test set: Average loss: 0.0569, Accuracy: 5508/5570 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0607, Accuracy: 5071/5140 (99%)\n",
      "\n",
      "[20,  1413] loss: 51.530 correct: 0.993\n",
      "\n",
      "Test set: Average loss: 0.0569, Accuracy: 5508/5570 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0608, Accuracy: 5069/5140 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_plan = Net(8).cuda()\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "epochs = 20\n",
    "optimizer_plan = torch.optim.AdamW(model_plan.parameters(), 3e-5)\n",
    "scheduler_plan = get_linear_schedule_with_warmup(optimizer_plan, 50, len(train_loader_plan)*epochs)\n",
    "for epoch  in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    model_plan.train()\n",
    "    for i, sample_batched in enumerate(train_loader_plan):\n",
    "        optimizer_plan.zero_grad()\n",
    "\n",
    "        lang_goal_instr, gt = sample_batched\n",
    "        target = torch.LongTensor(gt).cuda()\n",
    "        output = model_plan(lang_goal_instr.cuda())\n",
    "        loss =  F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer_plan.step()\n",
    "        scheduler_plan.step()\n",
    "        running_loss += loss.item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    print('[%d, %5d] loss: %.3f correct: %.3f' % (epoch + 1, i + 1, running_loss, correct/len(train_loader_plan.dataset) ))\n",
    "    # print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / len(train_loader.dataset)))\n",
    "            \n",
    "    \n",
    "    model_plan.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i, sample_batched in enumerate(valid_seen_loader_plan):\n",
    "           \n",
    "\n",
    "            lang_goal_instr, gt = sample_batched\n",
    "            target = torch.LongTensor(gt).cuda()\n",
    "            output = model_plan(lang_goal_instr.cuda())\n",
    "\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        test_loss /= len(valid_seen_loader_plan.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(valid_seen_loader_plan.dataset),\n",
    "    100. * correct / len(valid_seen_loader_plan.dataset)))\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i, sample_batched in enumerate(valid_unseen_loader_plan):\n",
    "          \n",
    "\n",
    "            lang_goal_instr, gt = sample_batched\n",
    "            target = torch.LongTensor(gt).cuda()\n",
    "            output = model_plan(lang_goal_instr.cuda())\n",
    "\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        test_loss /= len(valid_unseen_loader_plan.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(valid_unseen_loader_plan.dataset),\n",
    "    100. * correct / len(valid_unseen_loader_plan.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([0.0000, 0.5047, 0.0613, 0.0000, 0.0000, 0.0405, 0.0000, 0.0979, 0.3134,\n        0.1428, 0.1088, 0.2581, 0.3765, 0.3909, 0.0056, 0.0000, 0.0000, 0.0000,\n        0.0131, 0.0041, 0.0052, 0.7646, 0.3383, 0.1135, 0.0000, 0.0000, 0.6592,\n        0.0065, 0.3070, 0.0137, 0.0861, 0.4369, 0.0000, 0.0000, 0.3689, 0.0331,\n           nan, 0.0000, 0.2374, 0.2229, 0.0000, 0.0000, 0.2193, 0.2765, 0.0000,\n        0.0000, 0.0000, 0.2155, 0.1890, 0.0018, 0.0000, 0.0000, 0.1124, 0.0000,\n        0.0848, 0.6442, 0.2649, 0.0000, 0.0000, 0.1511, 0.0000, 0.6041, 0.1562,\n        0.0000, 0.0026, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2825, 0.0938,\n        0.0000, 0.6502, 0.0000, 0.0000, 0.0101])\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "index 79 is out of bounds for dimension 0 with size 77",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-362899102b56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiagonal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m79\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[%d, %5d] loss: %.3f correct: %.3f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader_plan_argument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / len(train_loader.dataset)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 79 is out of bounds for dimension 0 with size 77"
     ]
    }
   ],
   "source": [
    "num_class = 77\n",
    "model_plan_argument = Net(num_class).cuda()\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "epochs = 50\n",
    "optimizer_plan_argument = torch.optim.AdamW(model_plan_argument.parameters(), 3e-5)\n",
    "scheduler_plan_argument = get_linear_schedule_with_warmup(optimizer_plan_argument, 50, len(train_loader_plan_argument)*epochs)\n",
    "\n",
    "for epoch  in range(epochs):\n",
    "    confusion_matrix = torch.zeros(num_class, num_class)\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    model_plan_argument.train()\n",
    "    for i, sample_batched in enumerate(train_loader_plan_argument):\n",
    "        optimizer_plan_argument.zero_grad()\n",
    "\n",
    "        lang_goal_instr, gt = sample_batched\n",
    "        target = torch.LongTensor(gt).cuda()\n",
    "        output = model_plan_argument(lang_goal_instr.cuda())\n",
    "        loss =  F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer_plan_argument.step()\n",
    "        scheduler_plan_argument.step()\n",
    "        running_loss += loss.item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        \n",
    "        for t, p in zip(torch.tensor(gt).view(-1), pred.view(-1)):\n",
    "                confusion_matrix[t.long(), p.long()] += 1\n",
    "    print(confusion_matrix.diagonal()/confusion_matrix.sum(1))\n",
    "    # print(confusion_matrix[79])\n",
    "    print('[%d, %5d] loss: %.3f correct: %.3f' % (epoch + 1, i + 1, running_loss, correct/len(train_loader_plan_argument.dataset) ))\n",
    "    # print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / len(train_loader.dataset)))\n",
    "            \n",
    "    \n",
    "    model_plan_argument.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i, sample_batched in enumerate(valid_seen_loader_plan_argument):\n",
    "           \n",
    "\n",
    "            lang_goal_instr, gt = sample_batched\n",
    "            target = torch.LongTensor(gt).cuda()\n",
    "            output = model_plan_argument(lang_goal_instr.cuda())\n",
    "\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        test_loss /= len(valid_seen_loader_plan_argument.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(valid_seen_loader_plan_argument.dataset),\n",
    "    100. * correct / len(valid_seen_loader_plan_argument.dataset)))\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i, sample_batched in enumerate(valid_unseen_loader_plan_argument):\n",
    "          \n",
    "\n",
    "            lang_goal_instr, gt = sample_batched\n",
    "            target = torch.LongTensor(gt).cuda()\n",
    "            output = model_plan_argument(lang_goal_instr.cuda())\n",
    "\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        test_loss /= len(valid_unseen_loader_plan_argument.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(valid_unseen_loader_plan_argument.dataset),\n",
    "    100. * correct / len(valid_unseen_loader_plan_argument.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([2, 2])"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}