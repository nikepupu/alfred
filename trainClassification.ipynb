{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3613jvsc74a57bd031146510740782b1bc15b20863bd6d0a0dc62b272327c4675f88c0b4f1de465b",
   "display_name": "Python 3.6.13 64-bit ('alfred': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "31146510740782b1bc15b20863bd6d0a0dc62b272327c4675f88c0b4f1de465b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f4d5460b060>"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import json\n",
    "from gen.utils.py_util import remove_spaces_and_lower\n",
    "from vocab import Vocab\n",
    "import revtok\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "import models.nn.vnn as vnn\n",
    "import torch. nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset.json', 'r') as fp:\n",
    "    data_train = json.load(fp)\n",
    "with open('dataset_valid_seen.json', 'r') as fp:\n",
    "    data_valid_seen = json.load(fp)\n",
    "with open('dataset_valid_unseen.json', 'r') as fp:\n",
    "    data_valid_unseen = json.load(fp)\n",
    "with open('dataset'+'_high_plan_train'+'.json', 'r') as fp:\n",
    "    data_train_high_plan = json.load(fp)\n",
    "with open('dataset'+'_high_plan_valid_seen'+'.json', 'r') as fp:\n",
    "    data_valid_seen_high_plan = json.load(fp)\n",
    "with open('dataset'+'_high_plan_valid_unseen'+'.json', 'r') as fp:\n",
    "    data_valid_unseen_high_plan = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['look_at_obj_in_light']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['CleanObject', 'CoolObject', 'GotoLocation', 'HeatObject', 'PickupObject', 'PutObject', 'SliceObject', 'ToggleObject']\n",
      "['CleanObject', 'CoolObject', 'GotoLocation', 'HeatObject', 'PickupObject', 'PutObject', 'SliceObject', 'ToggleObject']\n",
      "['CleanObject', 'CoolObject', 'GotoLocation', 'HeatObject', 'PickupObject', 'PutObject', 'SliceObject', 'ToggleObject']\n"
     ]
    }
   ],
   "source": [
    "class HighPlanClassificationDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "      \n",
    "        self.dataset = dataset\n",
    "        self.label = list(dataset.keys())\n",
    "        self.label.sort()\n",
    "        print(self.label)\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.vocab = torch.load('pp.vocab')\n",
    "        self.pad = 0\n",
    "        self.seg = 1\n",
    "\n",
    "\n",
    "        for key in dataset.keys():\n",
    "            for item in dataset[key]:\n",
    "                traj = {}\n",
    "                traj['ann']  = {\n",
    "                    'goal': revtok.tokenize(remove_spaces_and_lower(item))\n",
    "                }\n",
    "                traj['num'] = {}\n",
    "                traj['num']['lang_goal'] = self.numericalize(self.vocab['word'], traj['ann']['goal'], train=True)\n",
    "            \n",
    "                lang_goal = traj['num']['lang_goal']\n",
    "                \n",
    "                # lang_goal_instr = lang_goal + lang_instr\n",
    "                # lang_goal_instr = lang_instr\n",
    "\n",
    "                lang_goal_instr = lang_goal\n",
    "                traj['num']['lang_goal_instr'] = lang_goal_instr\n",
    "\n",
    "                # for k, v in traj['num'].items():\n",
    "                #             if k in {'lang_goal_instr'}:\n",
    "                #                 # language embedding and padding\n",
    "                #                 seqs = [torch.tensor(vv, device=device) for vv in v]\n",
    "                #                 pad_seq = pad_sequence(seqs, batch_first=True, padding_value=self.pad)\n",
    "                #                 seq_lengths = np.array(list(map(len, v)))\n",
    "                #                 embed_seq = self.emb_word(pad_seq)\n",
    "                #                 packed_input = pack_padded_sequence(embed_seq, seq_lengths, batch_first=True, enforce_sorted=False)\n",
    "                #                 traj['num'][k] = packed_input\n",
    "\n",
    "                self.data.append( lang_goal_instr  )\n",
    "\n",
    "                self.labels.append(key)\n",
    "\n",
    "    def serialize_lang_action(self, feat):\n",
    "        '''\n",
    "        append segmented instr language and low-level actions into single sequences\n",
    "        '''\n",
    "        is_serialized = not isinstance(feat['num']['lang_instr'][0], list)\n",
    "        if not is_serialized:\n",
    "            feat['num']['lang_instr'] = [word for desc in feat['num']['lang_instr'] for word in desc]\n",
    "\n",
    "    @staticmethod\n",
    "    def numericalize(vocab, words, train=True):\n",
    "        '''\n",
    "        converts words to unique integers\n",
    "        '''\n",
    "        return vocab.word2index([w.strip().lower() for w in words], train=train)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "     \n",
    "        \n",
    "        return self.data[idx], self.label.index(self.labels[idx])\n",
    "\n",
    "train_plan = HighPlanClassificationDataset(data_train_high_plan)\n",
    "valid_seen_plan = HighPlanClassificationDataset(data_valid_seen_high_plan)\n",
    "valid_unseen_plan = HighPlanClassificationDataset(data_valid_unseen_high_plan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "([24, 19, 26, 39, 4, 97, 16], 2)"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "train_plan[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TaskClassificationDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "      \n",
    "        self.dataset = dataset\n",
    "        self.label = list(dataset.keys())\n",
    "        self.label.sort()\n",
    "        print(self.label)\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.vocab = torch.load('pp.vocab')\n",
    "        self.pad = 0\n",
    "        self.seg = 1\n",
    "\n",
    "\n",
    "        for key in dataset.keys():\n",
    "            for item in dataset[key]:\n",
    "                for p in item:\n",
    "                    traj = {}\n",
    "                    traj['ann']  = {\n",
    "                        'goal': revtok.tokenize(remove_spaces_and_lower(p['task_desc'])) + ['<<goal>>'],\n",
    "                        'instr': [revtok.tokenize(remove_spaces_and_lower(x)) for x in p['high_descs']] + [['<<stop>>']],\n",
    "                    }\n",
    "                    traj['num'] = {}\n",
    "                    traj['num']['lang_goal'] = self.numericalize(self.vocab['word'], traj['ann']['goal'], train=True)\n",
    "                    traj['num']['lang_instr'] = [self.numericalize(self.vocab['word'], x, train=True) for x in traj['ann']['instr']]\n",
    "                    self.serialize_lang_action(traj)\n",
    "                    lang_goal, lang_instr = traj['num']['lang_goal'], traj['num']['lang_instr']\n",
    "                    lang_goal_instr = lang_goal + lang_instr\n",
    "                    # lang_goal_instr = lang_instr\n",
    "                    # lang_goal_instr = lang_goal\n",
    "                    traj['num']['lang_goal_instr'] = lang_goal_instr\n",
    "                    # for k, v in traj['num'].items():\n",
    "                    #             if k in {'lang_goal_instr'}:\n",
    "                    #                 # language embedding and padding\n",
    "                    #                 seqs = [torch.tensor(vv, device=device) for vv in v]\n",
    "                    #                 pad_seq = pad_sequence(seqs, batch_first=True, padding_value=self.pad)\n",
    "                    #                 seq_lengths = np.array(list(map(len, v)))\n",
    "                    #                 embed_seq = self.emb_word(pad_seq)\n",
    "                    #                 packed_input = pack_padded_sequence(embed_seq, seq_lengths, batch_first=True, enforce_sorted=False)\n",
    "                    #                 traj['num'][k] = packed_input\n",
    "\n",
    "                    self.data.append( lang_goal_instr  )\n",
    "\n",
    "                    self.labels.append(key)\n",
    "\n",
    "    def serialize_lang_action(self, feat):\n",
    "        '''\n",
    "        append segmented instr language and low-level actions into single sequences\n",
    "        '''\n",
    "        is_serialized = not isinstance(feat['num']['lang_instr'][0], list)\n",
    "        if not is_serialized:\n",
    "            feat['num']['lang_instr'] = [word for desc in feat['num']['lang_instr'] for word in desc]\n",
    "\n",
    "    @staticmethod\n",
    "    def numericalize(vocab, words, train=True):\n",
    "        '''\n",
    "        converts words to unique integers\n",
    "        '''\n",
    "        return vocab.word2index([w.strip().lower() for w in words], train=train)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "     \n",
    "        \n",
    "        return self.data[idx], self.label.index(self.labels[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['look_at_obj_in_light', 'pick_and_place_simple', 'pick_and_place_with_movable_recep', 'pick_clean_then_place_in_recep', 'pick_cool_then_place_in_recep', 'pick_heat_then_place_in_recep', 'pick_two_obj_and_place']\n",
      "['look_at_obj_in_light', 'pick_and_place_simple', 'pick_and_place_with_movable_recep', 'pick_clean_then_place_in_recep', 'pick_cool_then_place_in_recep', 'pick_heat_then_place_in_recep', 'pick_two_obj_and_place']\n",
      "['look_at_obj_in_light', 'pick_and_place_simple', 'pick_and_place_with_movable_recep', 'pick_clean_then_place_in_recep', 'pick_cool_then_place_in_recep', 'pick_heat_then_place_in_recep', 'pick_two_obj_and_place']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_set = TaskClassificationDataset(data_train )\n",
    "val_set1 = TaskClassificationDataset(data_valid_seen )\n",
    "val_set2 = TaskClassificationDataset(data_valid_unseen)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_output=7):\n",
    "        super().__init__()\n",
    "        self.lang_dropout = nn.Dropout(0.3, inplace=True)\n",
    "        self.enc = nn.LSTM(512, 512, bidirectional=True, batch_first=True)\n",
    "        self.enc_att = vnn.SelfAttn(512 * 2)\n",
    "        self.fc1 = nn.Linear(1024,num_output)\n",
    "       \n",
    "        self.emb_word = nn.Embedding(len(train_set.vocab['word']), 512)\n",
    "    def forward(self, x):\n",
    "        emb_lang_goal_instr = x\n",
    "        self.lang_dropout(emb_lang_goal_instr.data)\n",
    "        enc_lang_goal_instr, _ = self.enc(emb_lang_goal_instr)\n",
    "        enc_lang_goal_instr, _ = pad_packed_sequence(enc_lang_goal_instr, batch_first=True)\n",
    "        self.lang_dropout(enc_lang_goal_instr)\n",
    "        cont_lang_goal_instr = self.enc_att(enc_lang_goal_instr)\n",
    "        res = self.fc1(cont_lang_goal_instr)\n",
    "        res = nn.functional.relu(res)\n",
    "\n",
    "        # res = self.fc2(res)\n",
    "        # res = nn.functional.relu(res)\n",
    "\n",
    "        res = F.log_softmax(res, dim=1)\n",
    "        return res\n",
    "model = Net().cuda()\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "        device = 'cuda'\n",
    "        lang_goal_instr, gt = zip(*batch)\n",
    "        seqs = [torch.tensor(vv, device=device) for vv in lang_goal_instr]\n",
    "        pad_seq = pad_sequence(seqs, batch_first=True, padding_value=0)\n",
    "        seq_lengths = np.array(list(map(len, lang_goal_instr)))\n",
    "        embed_seq = model.emb_word(pad_seq)\n",
    "        packed_input = pack_padded_sequence(embed_seq, seq_lengths, batch_first=True, enforce_sorted=False)\n",
    "        lang_goal_instr = packed_input\n",
    "        return lang_goal_instr, gt \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "21025\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "train_loader = DataLoader( train_set, batch_size=100, collate_fn=collate_fn, shuffle=True, num_workers=0)\n",
    "val_loader1 = DataLoader( val_set1, batch_size=100, collate_fn=collate_fn, shuffle=False, num_workers=0)\n",
    "val_loader2 = DataLoader( val_set2, batch_size=100, collate_fn=collate_fn, shuffle=False, num_workers=0)\n",
    "\n",
    "train_loader_plan = DataLoader( train_plan, batch_size=100, collate_fn=collate_fn, shuffle=True, num_workers=0)\n",
    "valid_seen_loader_plan = DataLoader( valid_seen_plan, batch_size=100, collate_fn=collate_fn, shuffle=True, num_workers=0)\n",
    "valid_unseen_loader_plan = DataLoader( valid_unseen_plan, batch_size=100, collate_fn=collate_fn, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(num_output=7).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1,   211] loss: 370.307 correct: 0.339\n",
      "\n",
      "Test set: Average loss: 1.0698, Accuracy: 503/820 (61%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.9294, Accuracy: 574/821 (70%)\n",
      "\n",
      "[2,   211] loss: 136.085 correct: 0.774\n",
      "\n",
      "Test set: Average loss: 0.4533, Accuracy: 691/820 (84%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.3836, Accuracy: 698/821 (85%)\n",
      "\n",
      "[3,   211] loss: 69.973 correct: 0.894\n",
      "\n",
      "Test set: Average loss: 0.3283, Accuracy: 728/820 (89%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.3159, Accuracy: 728/821 (89%)\n",
      "\n",
      "[4,   211] loss: 38.625 correct: 0.951\n",
      "\n",
      "Test set: Average loss: 0.1752, Accuracy: 771/820 (94%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1582, Accuracy: 782/821 (95%)\n",
      "\n",
      "[5,   211] loss: 26.477 correct: 0.966\n",
      "\n",
      "Test set: Average loss: 0.1413, Accuracy: 789/820 (96%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1173, Accuracy: 790/821 (96%)\n",
      "\n",
      "[6,   211] loss: 20.421 correct: 0.975\n",
      "\n",
      "Test set: Average loss: 0.1409, Accuracy: 787/820 (96%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1191, Accuracy: 788/821 (96%)\n",
      "\n",
      "[7,   211] loss: 16.937 correct: 0.979\n",
      "\n",
      "Test set: Average loss: 0.1345, Accuracy: 789/820 (96%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1124, Accuracy: 791/821 (96%)\n",
      "\n",
      "[8,   211] loss: 14.862 correct: 0.981\n",
      "\n",
      "Test set: Average loss: 0.1115, Accuracy: 790/820 (96%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0984, Accuracy: 796/821 (97%)\n",
      "\n",
      "[9,   211] loss: 12.801 correct: 0.985\n",
      "\n",
      "Test set: Average loss: 0.1051, Accuracy: 798/820 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0927, Accuracy: 800/821 (97%)\n",
      "\n",
      "[10,   211] loss: 11.624 correct: 0.985\n",
      "\n",
      "Test set: Average loss: 0.1131, Accuracy: 794/820 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0926, Accuracy: 797/821 (97%)\n",
      "\n",
      "[11,   211] loss: 10.510 correct: 0.988\n",
      "\n",
      "Test set: Average loss: 0.0772, Accuracy: 804/820 (98%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0866, Accuracy: 800/821 (97%)\n",
      "\n",
      "[12,   211] loss: 10.286 correct: 0.987\n",
      "\n",
      "Test set: Average loss: 0.1185, Accuracy: 792/820 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1048, Accuracy: 798/821 (97%)\n",
      "\n",
      "[13,   211] loss: 9.197 correct: 0.989\n",
      "\n",
      "Test set: Average loss: 0.1282, Accuracy: 794/820 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1128, Accuracy: 798/821 (97%)\n",
      "\n",
      "[14,   211] loss: 8.697 correct: 0.990\n",
      "\n",
      "Test set: Average loss: 0.1051, Accuracy: 796/820 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0973, Accuracy: 799/821 (97%)\n",
      "\n",
      "[15,   211] loss: 8.245 correct: 0.990\n",
      "\n",
      "Test set: Average loss: 0.1106, Accuracy: 794/820 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0961, Accuracy: 799/821 (97%)\n",
      "\n",
      "[16,   211] loss: 8.013 correct: 0.990\n",
      "\n",
      "Test set: Average loss: 0.1025, Accuracy: 799/820 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0916, Accuracy: 801/821 (98%)\n",
      "\n",
      "[17,   211] loss: 7.689 correct: 0.991\n",
      "\n",
      "Test set: Average loss: 0.1069, Accuracy: 797/820 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0983, Accuracy: 800/821 (97%)\n",
      "\n",
      "[18,   211] loss: 7.489 correct: 0.991\n",
      "\n",
      "Test set: Average loss: 0.1114, Accuracy: 797/820 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1016, Accuracy: 800/821 (97%)\n",
      "\n",
      "[19,   211] loss: 7.220 correct: 0.991\n",
      "\n",
      "Test set: Average loss: 0.0980, Accuracy: 799/820 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0933, Accuracy: 800/821 (97%)\n",
      "\n",
      "[20,   211] loss: 6.970 correct: 0.992\n",
      "\n",
      "Test set: Average loss: 0.1009, Accuracy: 797/820 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0952, Accuracy: 800/821 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "epochs = 20\n",
    "optimizer = torch.optim.AdamW(model.parameters(), 3e-5)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, 50, len(train_loader)*epochs)\n",
    "for epoch  in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    model.train()\n",
    "    for i, sample_batched in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        lang_goal_instr, gt = sample_batched\n",
    "        target = torch.LongTensor(gt).cuda()\n",
    "        output = model(lang_goal_instr.cuda())\n",
    "        loss =  F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        running_loss += loss.item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    print('[%d, %5d] loss: %.3f correct: %.3f' % (epoch + 1, i + 1, running_loss, correct/len(train_loader.dataset) ))\n",
    "    # print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / len(train_loader.dataset)))\n",
    "            \n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i, sample_batched in enumerate(val_loader1):\n",
    "           \n",
    "\n",
    "            lang_goal_instr, gt = sample_batched\n",
    "            target = torch.LongTensor(gt).cuda()\n",
    "            output = model(lang_goal_instr.cuda())\n",
    "\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        test_loss /= len(val_loader1.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(val_loader1.dataset),\n",
    "    100. * correct / len(val_loader1.dataset)))\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i, sample_batched in enumerate(val_loader2):\n",
    "          \n",
    "\n",
    "            lang_goal_instr, gt = sample_batched\n",
    "            target = torch.LongTensor(gt).cuda()\n",
    "            output = model(lang_goal_instr.cuda())\n",
    "\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        test_loss /= len(val_loader2.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(val_loader2.dataset),\n",
    "    100. * correct / len(val_loader2.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1,  1413] loss: 591.551 correct: 0.881\n",
      "\n",
      "Test set: Average loss: 0.1374, Accuracy: 5368/5570 (96%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1565, Accuracy: 4936/5140 (96%)\n",
      "\n",
      "[2,  1413] loss: 159.562 correct: 0.976\n",
      "\n",
      "Test set: Average loss: 0.1142, Accuracy: 5476/5570 (98%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1238, Accuracy: 5039/5140 (98%)\n",
      "\n",
      "[3,  1413] loss: 107.833 correct: 0.986\n",
      "\n",
      "Test set: Average loss: 0.0706, Accuracy: 5492/5570 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0783, Accuracy: 5063/5140 (99%)\n",
      "\n",
      "[4,  1413] loss: 76.420 correct: 0.990\n",
      "\n",
      "Test set: Average loss: 0.0677, Accuracy: 5498/5570 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0744, Accuracy: 5068/5140 (99%)\n",
      "\n",
      "[5,  1413] loss: 71.349 correct: 0.991\n",
      "\n",
      "Test set: Average loss: 0.0646, Accuracy: 5500/5570 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0709, Accuracy: 5069/5140 (99%)\n",
      "\n",
      "[6,  1413] loss: 67.647 correct: 0.991\n",
      "\n",
      "Test set: Average loss: 0.0623, Accuracy: 5505/5570 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0683, Accuracy: 5069/5140 (99%)\n",
      "\n",
      "[7,  1413] loss: 65.607 correct: 0.991\n",
      "\n",
      "Test set: Average loss: 0.0610, Accuracy: 5505/5570 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0682, Accuracy: 5071/5140 (99%)\n",
      "\n",
      "[8,  1413] loss: 62.922 correct: 0.991\n",
      "\n",
      "Test set: Average loss: 0.0603, Accuracy: 5505/5570 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0658, Accuracy: 5068/5140 (99%)\n",
      "\n",
      "[9,  1413] loss: 60.987 correct: 0.992\n",
      "\n",
      "Test set: Average loss: 0.0598, Accuracy: 5505/5570 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0648, Accuracy: 5070/5140 (99%)\n",
      "\n",
      "[10,  1413] loss: 59.571 correct: 0.992\n",
      "\n",
      "Test set: Average loss: 0.0594, Accuracy: 5507/5570 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0643, Accuracy: 5072/5140 (99%)\n",
      "\n",
      "[11,  1413] loss: 58.370 correct: 0.992\n",
      "\n",
      "Test set: Average loss: 0.0589, Accuracy: 5508/5570 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0631, Accuracy: 5073/5140 (99%)\n",
      "\n",
      "[12,  1413] loss: 56.732 correct: 0.992\n",
      "\n",
      "Test set: Average loss: 0.0581, Accuracy: 5508/5570 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0640, Accuracy: 5073/5140 (99%)\n",
      "\n",
      "[13,  1413] loss: 56.080 correct: 0.992\n",
      "\n",
      "Test set: Average loss: 0.0581, Accuracy: 5508/5570 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0626, Accuracy: 5068/5140 (99%)\n",
      "\n",
      "[14,  1413] loss: 55.055 correct: 0.992\n",
      "\n",
      "Test set: Average loss: 0.0576, Accuracy: 5509/5570 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0623, Accuracy: 5068/5140 (99%)\n",
      "\n",
      "[15,  1413] loss: 54.348 correct: 0.993\n",
      "\n",
      "Test set: Average loss: 0.0575, Accuracy: 5508/5570 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0621, Accuracy: 5070/5140 (99%)\n",
      "\n",
      "[16,  1413] loss: 53.121 correct: 0.993\n",
      "\n",
      "Test set: Average loss: 0.0570, Accuracy: 5510/5570 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0622, Accuracy: 5072/5140 (99%)\n",
      "\n",
      "[17,  1413] loss: 52.895 correct: 0.993\n",
      "\n",
      "Test set: Average loss: 0.0571, Accuracy: 5508/5570 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0613, Accuracy: 5069/5140 (99%)\n",
      "\n",
      "[18,  1413] loss: 52.294 correct: 0.993\n",
      "\n",
      "Test set: Average loss: 0.0570, Accuracy: 5508/5570 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0616, Accuracy: 5068/5140 (99%)\n",
      "\n",
      "[19,  1413] loss: 51.994 correct: 0.993\n",
      "\n",
      "Test set: Average loss: 0.0569, Accuracy: 5508/5570 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0607, Accuracy: 5071/5140 (99%)\n",
      "\n",
      "[20,  1413] loss: 51.530 correct: 0.993\n",
      "\n",
      "Test set: Average loss: 0.0569, Accuracy: 5508/5570 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0608, Accuracy: 5069/5140 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_plan = Net(8).cuda()\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "epochs = 20\n",
    "optimizer_plan = torch.optim.AdamW(model_plan.parameters(), 3e-5)\n",
    "scheduler_plan = get_linear_schedule_with_warmup(optimizer_plan, 50, len(train_loader_plan)*epochs)\n",
    "for epoch  in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    model_plan.train()\n",
    "    for i, sample_batched in enumerate(train_loader_plan):\n",
    "        optimizer_plan.zero_grad()\n",
    "\n",
    "        lang_goal_instr, gt = sample_batched\n",
    "        target = torch.LongTensor(gt).cuda()\n",
    "        output = model_plan(lang_goal_instr.cuda())\n",
    "        loss =  F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer_plan.step()\n",
    "        scheduler_plan.step()\n",
    "        running_loss += loss.item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    print('[%d, %5d] loss: %.3f correct: %.3f' % (epoch + 1, i + 1, running_loss, correct/len(train_loader_plan.dataset) ))\n",
    "    # print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / len(train_loader.dataset)))\n",
    "            \n",
    "    \n",
    "    model_plan.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i, sample_batched in enumerate(valid_seen_loader_plan):\n",
    "           \n",
    "\n",
    "            lang_goal_instr, gt = sample_batched\n",
    "            target = torch.LongTensor(gt).cuda()\n",
    "            output = model_plan(lang_goal_instr.cuda())\n",
    "\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        test_loss /= len(valid_seen_loader_plan.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(valid_seen_loader_plan.dataset),\n",
    "    100. * correct / len(valid_seen_loader_plan.dataset)))\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i, sample_batched in enumerate(valid_unseen_loader_plan):\n",
    "          \n",
    "\n",
    "            lang_goal_instr, gt = sample_batched\n",
    "            target = torch.LongTensor(gt).cuda()\n",
    "            output = model_plan(lang_goal_instr.cuda())\n",
    "\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        test_loss /= len(valid_unseen_loader_plan.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(valid_unseen_loader_plan.dataset),\n",
    "    100. * correct / len(valid_unseen_loader_plan.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[6],\n",
       "        [6],\n",
       "        [6],\n",
       "        [6],\n",
       "        [6],\n",
       "        [6],\n",
       "        [6],\n",
       "        [6],\n",
       "        [6],\n",
       "        [6],\n",
       "        [6],\n",
       "        [6],\n",
       "        [6],\n",
       "        [6],\n",
       "        [6],\n",
       "        [6],\n",
       "        [6],\n",
       "        [5],\n",
       "        [5],\n",
       "        [6],\n",
       "        [6]], device='cuda:0')"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([2, 2])"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}